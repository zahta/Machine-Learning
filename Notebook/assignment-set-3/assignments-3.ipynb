{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The solutions of exercises of the book \"Understanding Machine Learning-from Theory to Algorithms\"      \n",
    "by [Zahra Taheri](https://github.com/zata213/Applied_Machine_Learning_S20_Assignments)\n",
    "\n",
    "#### Chapter 2 (A Gentle Start)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $2.1$**\n",
    "\n",
    "Let $\\mathcal{f}$ be the labeling function. We want to show that given a training set $S$ as follows \n",
    "$$S=((x_i,\\mathcal{f}(x_i))) _{i=1}^m\\subseteq (\\mathbb{R}^d\\times \\{0,1\\}) ^m,$$\n",
    "there exists a polynomial $p_{_S}$ such that $h_{_S}(x)=1$ if and only if $p_{_S}(x)\\geq 0$, where $h_{_S}:\\mathcal{X}\\rightarrow \\mathcal{Y}$\n",
    "is the following predictor:\n",
    "\n",
    "\\begin{equation*} \n",
    "h_{_S}(x)=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "y_i & \\mbox{if $\\exists i \\in [m]$ s. t. $x=x_i$}\\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{array} \n",
    " \\right.\n",
    "\\end{equation*}\n",
    "\n",
    "where $[m]=\\{1,\\ldots,m\\}$ and $y_i=\\mathcal{f}(x_i)$, for all $i\\in [m]$.\n",
    "\n",
    "Obviously, we have:\n",
    "\\begin{equation*} \n",
    "h_{_S}(x)=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\mbox{if $\\exists i \\in [m]$ s. t. $x=x_i$ and $y_i=1$}\\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{array} \n",
    " \\right.\n",
    "\\end{equation*}\n",
    "\n",
    "It is easy to see that if $y_i=0$, for all $i \\in [m]$, then $h_{_S}(x)=0$, for all $x\\in \\mathcal{X}$. In this case, if $p_{_S}(x):=-1$, for all $x\\in \\mathcal{X}$, then the statement is obviously true. \n",
    "\n",
    "Suppose that there exists an $i$ in $[m]$ such that $y_i=1$.\n",
    "\n",
    "1. Let $m=1$ and $S=((x_1,1))$. Then $h_{_S}(x_1)=1$ and $h_{_S}(x)=0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1\\}$. So we must have $p_{_S}(x_1)\\geq 0$ and $p_{_S}(x)<0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1\\}$. \n",
    "Let $p_{_S}(x):=-\\Vert x-x_1\\Vert^2$. Such a definition helps us to check the distance between $x$ and $x_1$. It is easy to see that $p_{_S}(x_1)=0$ and $p_{_S}(x)<0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1\\}$.\n",
    "\n",
    "2. Let $m=2$. Then without loss of generality we may assume that $S=((x_1,1),(x_2,0))$ or $S=((x_1,1),(x_2,1))$. \n",
    "   * If $S=((x_1,1),(x_2,0))$, then $h_{_S}(x_1)=1$ and $h_{_S}(x)=0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1\\}$. So, similar to the case (1), by defining $p_{_S}(x):=-\\Vert x-x_1\\Vert^2$, the statement is obviously true.\n",
    "   * If $S=((x_1,1),(x_2,1))$, then $h_{_S}(x_1)=h_{_S}(x_2)=1$ and $h_{_S}(x)=0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1,x_2\\}$. So we must have $p_{_S}(x_1)\\geq 0$, $p_{_S}(x_2)\\geq 0$ and $p_{_S}(x)<0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1,x_2\\}$. \n",
    "With a similar discussion as above, let $p_{_S}(x):=-(\\Vert x-x_1\\Vert^2)(\\Vert x-x_1\\Vert^2)$. Then $p_{_S}(x)$ is a polynomial such that $p_{_S}(x_1)=p_{_S}(x_2)=0$ and $p_{_S}(x)<0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1,x_2\\}$, and so the statement is obviously true. \n",
    "\n",
    "Inductively, we can generalize the obtained polynomials in cases (1) and (2) as follows:\n",
    "$$p_{_S}(x):=-\\prod_{i\\in [m]\\\\ \\ y_i=1}\\Vert x-x_i\\Vert^2$$\n",
    "Then $p_{_S}(x)$ is a polynomial such that $p_{_S}(x_i)=0$, for all $i \\in [m]$ s. t. $y_i=1$, and $p_{_S}(x)<0$, for all $x\\in \\mathcal{X}\\setminus\\{x_i|i\\in [m] \\text{ and } y_i=1\\}$, and so the statement is obviously true. \n",
    "\n",
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $2.2$**\n",
    "\n",
    "Let $\\mathcal{H}$ be a class of binary classifiers over a domain $\\mathcal{X}$ and $\\mathcal{D}$ be a probability distribution over $\\mathcal{X}$. Let $\\mathcal{f}$ be the target hypothesis in $\\mathcal{H}$. Let $S=((x_i,\\mathcal{f}(x_i))) _{i=1}^m$ be a training set such that $x_1,\\ldots,x_m$ are i.i.d with respect to $\\mathcal{D}$, denoted by $S|_{\\mathcal{X}}\\sim\\mathcal{D}^m$. Fix some $h\\in\\mathcal{H}$.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{E}_{S|_{\\mathcal{X}}\\sim\\mathcal{D}^m}[L_{_S}(h)] & \\stackrel{*}{=} & \\dfrac{1}{m}\\sum_{i=1}^m\\mathbb{E}_{{x_i}\\sim{\\mathcal{D}}}[\\mathbb{1}_{h(x_i)\\not=f(x_i)}]\\\\\n",
    "& \\stackrel{**}{=} &\\dfrac{1}{m}\\sum_{i=1}^m\\mathbb{E}_{{x}\\sim\\mathcal{D}}[\\mathbb{1}_{h(x)\\not=f(x)}]\\\\\n",
    "& = & m \\left(\\frac{1}{m}\\right) \\mathbb{P}_{{x}\\sim\\mathcal{D}}[h(x)\\not=f(x)]\\\\\n",
    "& = & L_{(\\mathcal{D},\\mathcal{f})}[h]\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\\* Expectation is linear          \n",
    "\\** $x_1,\\ldots,x_m$ are i.i.d\n",
    "\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $2.3$**\n",
    "\n",
    "Let $S=((x_i,y_i)) _{i=1}^m$ be a training set.\n",
    "\n",
    "1. Let $R(S)$ be the rectangle returned by $A$ and $h_{A(S)}:\\mathcal{X}\\rightarrow \\mathcal{Y}$ be the corresponding hypothesis. Since $A$ returns the rectangle enclosing all positive examples in the training set, $h_{A(S)}(x_i)=1$, for all $i\\in[m]$ such that $y_i=1$. On the other hand, by the realizability assumption, there exists $h^*\\in \\mathcal{H}_\\text{rec}^2$ such that $L_S(h^*)=0$, and so $h^*(x_i)=1$, for all $i\\in[m]$ such that $y_i=1$. Since $A$ returns the smallest rectangle enclosing all positive examples, $L_S(h_{A(S)})=0$ and so $A$ is an ERM.\n",
    "\n",
    "2. Let $\\mathcal{D}$ be a probability distribution over $\\mathcal{X}$. Also let $R^*=(a_1^*,b_1^*,a_2^*,b_2^*)$ be the rectangle that generates the labels and $\\mathcal{f}$ be its corresponding hypothesis. Suppose that $R_1,\\ldots,R_4$ are defined as in the hint of this exercise. By the definitions of $R(S)$ and $R^*$ we have $R(S)\\subseteq R^*$.     \n",
    "By definitions,\n",
    "$$L_{(\\mathcal{D,f})}(h_{A(S)})=\\mathcal{D}(\\{x\\in \\mathcal{X}: h_{A(S)}(x)\\not=f(x)\\})=\\mathcal{D}(\\{x\\in\\mathcal{X}:x\\notin S|_{\\mathcal{X}} \\text{ and }f(x)=1\\})=\\mathcal{D}(R^*\\setminus R(S)).$$\n",
    "Since, the probability mass of the rectangle $R_i$ is exactly $\\frac{\\varepsilon}{4}$, for all $i\\in\\{1,2,3,4\\}$, if $S$ contains (positive) examples in all of the rectangles $R_1,\\ldots,R_4$, then $\\mathcal{D}(R^*\\setminus R(S))\\leq 4(\\frac{\\varepsilon}{4})=\\varepsilon$. Therefore, $L_{(\\mathcal{D,f})}(h_{A(S)})\\leq \\varepsilon$.    \n",
    "Now, we would like to upper bound $\\mathcal{D}^m(\\{S|_{\\mathcal{X}}:L_{(\\mathcal{D,f})}(h_S)> \\varepsilon\\})$. With the discussion above, if $S$ contains (positive) examples in all of the rectangles $R_1,\\ldots,R_4$, then $L_{(\\mathcal{D,f})}(h_{A(S)})\\leq \\varepsilon$. Therefore, \n",
    "$$\n",
    "\\{S|_{\\mathcal{X}}:L_{(\\mathcal{D,f})}(h_S)> \\varepsilon\\}=\\bigcup_{i=1}^4{\\{S|_{\\mathcal{X}}:S|_{\\mathcal{X}}\\cap R_i=\\varnothing\\}}.\n",
    "$$\n",
    "It is easy to see that $\\mathcal{D}^m(\\{S|_{\\mathcal{X}}:S|_{\\mathcal{X}}\\cap R_i=\\varnothing\\})=(1-\\frac{\\varepsilon}{4})^m\\leq e^{-\\frac{\\varepsilon}{4} m}$, for all $i\\in\\{1,2,3,4\\}$.    \n",
    "With the discussion above and the union bound, $\\mathcal{D}^m(\\{S|_{\\mathcal{X}}:L_{(\\mathcal{D,f})}(h_S)> \\varepsilon\\})\\leq \\sum_{i=1}^4 e^{-\\frac{\\varepsilon}{4} m}=4e^{-\\frac{\\varepsilon}{4} m}$. So, the assumption $m\\geq \\frac{4 \\log (4/\\delta)}{\\varepsilon}$ completes the proof.\n",
    "\n",
    "3. Similar to the definition of axis aligned rectangles in $\\mathbb{R}^2$, given real numbers $a_1\\leq b_1,\\ldots, a_d\\leq b_d$, define the classifier $h_{(a_1, b_1,\\ldots, a_d, b_d)}$ as follows:\n",
    "\\begin{equation*} \n",
    "h_{(a_1, b_1,\\ldots, a_d, b_d)}(x_1,\\ldots,x_d)=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\mbox{if $a_i\\leq x_i\\leq b_i$, for all $i\\in [d]$}\\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{array} \n",
    " \\right.\n",
    "\\end{equation*}\n",
    "Also, the class of all axis aligned rectangles in $\\mathbb{R}^d$ is defined as follows:\n",
    "$$\\mathcal{H}_\\text{rec}^d=\\{h_{(a_1, b_1,\\ldots, a_d, b_d)}:a_1\\leq b_1,\\ldots, a_d\\leq b_d\\}.$$\n",
    "For $i\\in[2d]$, we define rectangles $R_i$ similar to $R_1,\\ldots,R_4$ in the latter case, with the probability mass $\\frac{\\varepsilon}{2d}$. By generalizing the algorithm $A$ to the case $\\mathbb{R}^d$, the proofs of parts (1) and (2) are straightforward (by considering a training set of size $\\geq \\frac{2d \\log (2d/\\delta)}{\\varepsilon}$).\n",
    "---------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
