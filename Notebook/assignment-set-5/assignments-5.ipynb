{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The solutions of some exercises of the book \"Understanding Machine Learning-from Theory to Algorithms\"      \n",
    "by [Zahra Taheri](https://github.com/zata213/Applied_Machine_Learning_S20_Assignments)\n",
    "\n",
    "#### Chapter 3 (A Formal Learning Model)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $3.2$**\n",
    "\n",
    "1. As it is mentioned in the exercise, the realizability assumption here implies that the true hypothesis $\\mathcal{f}$ labels negatively all examples in the domain $\\mathcal{X}$, perhaps except one. Let $A$ be the algorithm that returns an hypothesis $h_S$ with the following property:\n",
    "\\begin{equation*} \n",
    "h_{_S}=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "h_x & \\mbox{if $\\exists x\\in S$   s. t.   $\\mathcal{f}(x)=1$}\\\\\n",
    "h^{-} & \\mbox{otherwise}\n",
    "\\end{array} \n",
    " \\right.\n",
    "\\end{equation*}\n",
    "It is easy to see that $L_S(h_S)=0$, and so $A$ is an $ERM$.\n",
    "\n",
    "2. Let $\\mathcal{D}$ be a probability distribution over $\\mathcal{X}$ and $\\varepsilon\\in(0,1)$. If $\\mathcal{f}=h^{-}$, then $A$ returns the true hypothesis. Suppose that there exists an $x$ in $\\mathcal{X}$ such that $\\mathcal{f}(x)=1$. Such an element is unique, by the realizability assumption. Let $S|_{\\mathcal{X}}= (x_1,\\ldots,x_m)$ be the instances of the training set. We would like to upper bound $\\mathcal{D}^m(\\{S|_{\\mathcal{X}}:L_{(\\mathcal{D,f})}(h_S)> \\varepsilon\\})$. If $x\\in S|_{\\mathcal{X}}$, then $A$ returns the true hypothesis and so, $L_{(\\mathcal{D,f})}(h_S)=0$. Therefore we are interested in cases that $x\\not\\in S|_{\\mathcal{X}}$. Also, if $\\mathcal{D}(x)\\leq \\varepsilon$, then $L_{(\\mathcal{D,f})}(h)\\leq \\varepsilon$, for all $h$ in the hypothesis class. So, we should suppose that $\\mathcal{D}(x)> \\varepsilon$. Then, $\\mathcal{D}(x')\\leq 1-\\varepsilon$, for all $x' \\in \\mathcal{X}\\setminus{x}$. Hence we have \n",
    "$$\\{S|_{\\mathcal{X}}:L_{(\\mathcal{D,f})}(h_S)> \\varepsilon\\}=\\{S|_{\\mathcal{X}}:x\\not\\in S|_{\\mathcal{X}} \\text{ and } \\mathcal{D}(x)> \\varepsilon\\}=\\{S|_{\\mathcal{X}}:\\forall x'\\in S|_{\\mathcal{X}} \\ \\ \\ \\mathcal{D}(x')\\leq 1-\\varepsilon\\}.$$\n",
    "Therefore we have:\n",
    "$$\\mathcal{D}^m(\\{S|_{\\mathcal{X}}:L_{(\\mathcal{D,f})}(h_S)> \\varepsilon\\})=\\mathcal{D}^m(\\{S|_{\\mathcal{X}}:\\forall x'\\in S|_{\\mathcal{X}} \\ \\ \\mathcal{D}(x')\\leq 1-\\varepsilon\\})\\\\ \\leq (1-\\varepsilon)^m\\leq e^{-\\varepsilon m}.$$\n",
    "Let $\\delta \\in (0,1)$ such that $e^{-\\varepsilon m}\\leq \\delta$. So, $m\\geq \\frac{log({1}/{\\delta})}{\\varepsilon}$. Therefore, $\\mathcal{H}_{singleton}$ is PAC learnable with $m_{\\mathcal{H}_{singleton}}\\leq \\lceil\\frac{log({1}/{\\delta})}{\\varepsilon}\\rceil$.\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $3.3$**\n",
    "\n",
    "Similar to the Exercise $3$ of Chapter $2$, let $A$ be the algorithm that returns the smallest circle enclosing all positive examples in the training set $S$. Let $C(S)$ be the circle returned by $A$ with the radius $r(S)$ and $h_{A(S)}:\\mathcal{X}\\rightarrow \\mathcal{Y}$ be the corresponding hypothesis. Similar to the Exercise $2.3$, it is easy to see that $L_S(h_{A(S)})=0$ and so $A$ is an ERM.\n",
    "\n",
    "Let $\\mathcal{D}$ be a probability distribution over $\\mathcal{X}$, $\\varepsilon\\in(0,1)$, and $\\mathcal{f}$ be the target hypothesis in $\\mathcal{H}$. By the realizability assumption, there exists a circle $C^*$ with the radius $r^*$ and the corresponding hypothesis $h^*$ related to the zero generalization error. By the definitions of $C(S)$ and $C^*$ we have $C(S)\\subseteq C^*$. Also we have:\n",
    "$$L_{(\\mathcal{D,f})}(h_{A(S)})=\\mathcal{D}(\\{x\\in \\mathcal{X}: h_{A(S)}(x)\\not=\\mathcal{f}(x)\\})=\\mathcal{D}(\\{x\\in\\mathcal{X}:x\\notin S|_{\\mathcal{X}} \\text{ and }\\mathcal{f}(x)=1\\})=\\mathcal{D}(C^*\\setminus C(S)).$$\n",
    "Let $r_1\\leq r^*$  be a number such that the probability mass (with respect to $\\mathcal{D}$) of the strip $C_1=\\{x\\in \\mathcal{R}^2:r_1\\leq\\Vert x\\Vert \\leq r^*\\}$ is $\\varepsilon$. Since $L_{(\\mathcal{D,f})}(h_{A(S)})=\\mathcal{D}(C^*\\setminus C(S))$, by the discussion above we have $L_{(\\mathcal{D,f})}(h_{A(S)})\\leq \\varepsilon$.     \n",
    "Now, we would like to upper bound $\\mathcal{D}^m(\\{S|_{\\mathcal{X}}:L_{(\\mathcal{D,f})}(h_S)> \\varepsilon\\})$. With the discussion above, \n",
    "$$\n",
    "\\{S|_{\\mathcal{X}}:L_{(\\mathcal{D,f})}(h_S)> \\varepsilon\\}=\\{S|_{\\mathcal{X}}:S|_{\\mathcal{X}}\\cap C_1=\\varnothing\\}.\n",
    "$$\n",
    "Therefore we have :\n",
    "$$\\mathcal{D}^m(\\{S|_{\\mathcal{X}}:L_{(\\mathcal{D,f})}(h_S)> \\varepsilon\\})\\leq (1-\\varepsilon)^m\\leq e^{-\\varepsilon m}$$\n",
    "Let $\\delta \\in (0,1)$ such that $e^{-\\varepsilon m}\\leq \\delta$. So, $m\\geq \\frac{log({1}/{\\delta})}{\\varepsilon}$. Therefore, $\\mathcal{H}$ is PAC learnable with $m_{\\mathcal{H}}\\leq \\lceil\\frac{log({1}/{\\delta})}{\\varepsilon}\\rceil$.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $3.4$**\n",
    "\n",
    "1. Let $\\mathcal{H}$ be the hypothesis class of all conjunctions over $d$ variables. If we show that $\\mathcal{H}$ is finite, then by corollary $3.2$, $\\mathcal{H}$ is PAC learnable. Let $h\\in \\mathcal{H}$ and $h$ is not the all-negative hypothesis. Let $x=(x_1,\\ldots,x_d)\\in \\mathcal{X}$. Then $h(x)=\\bigwedge_{i=1}^d{a_i}$, where $a_i\\in \\{x_i,\\bar{x_i},none\\}$, for all $i\\in d$, in which by $none$ we means that the literals $x_i$ and $\\bar{x_i}$ are not appear in $h(x)$. Therefore, $|\\mathcal{H}|=3^d+1$ and so by corollary $3.2$, $\\mathcal{H}$ is PAC learnable with sample complexity\n",
    "$$m_{\\mathcal{H}}(\\varepsilon,\\delta)\\leq \\lceil\\frac{log({|\\mathcal{H}|}/{\\delta})}{\\varepsilon}\\rceil.$$\n",
    "\n",
    "2. Suppose that $S$ is a training set of size $m$ such that $x_1',\\ldots,x_l'$ are all positively labeled instances in $S$. By induction on $i\\leq l$, we define conjunctions $h_i$. Let $h_0$ be the all-negative hypothesis with definition $h_0(x):=\\bigwedge_{j=1}^d{x_j\\overline{x_j}}$. Let ${i+1}\\leq l$ and $x_{i+1}'=(x_1^{i+1},\\ldots,x_d^{i+1})$. We obtain $h_{i+1}$ from $h_i$ as follows:\n",
    "    * For all $j\\in [d]$, if $x_j^{i+1}=1$ and $\\overline{x_j^{i+1}}$ is a literal of $h_i$ then delete $\\overline{x_j^{i+1}}$.\n",
    "    *  For all $j\\in [d]$, if $x_j^{i+1}=0$ and $x_j^{i+1}$ is a literal of $h_i$ then delete $x_j^{i+1}$.  \n",
    "    \n",
    "   The algorithm returns $h_l$. It is easy to see that $h_l$ labels $x_1',\\ldots,x_l'$ as positive. Since $h_l$ is the most restrictive conjunction that labels positively all the positively labeled members of $S$ and by the realizability assumption, $L_S(h_l)=0$ and so the algorithm implements the ERM rule.    \n",
    "\n",
    "(Note: The solution of this exercise is explained in Section $8.2.3$ of the book)\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $3.5$**\n",
    "\n",
    "Let $\\mathcal{H}_B=\\{h\\in \\mathcal{H}:L_{(\\mathcal{\\overline{D}_m,f})}(h)>\\varepsilon\\}$ and $M=\\{{S|_{\\mathcal{X}}}:\\exists h\\in \\mathcal{H}_B \\ \\ s.t. \\ \\ L_{(S,f)}(h)=0\\}$. Then we have $\\mathbb{P}\\left[\\exists h\\in \\mathcal{H} \\ s.t. \\ L_{(\\mathcal{\\overline{D}_m,f})}(h)>\\varepsilon \\ and \\ L_{(S,f)}(h)=0\\right]=\\mathbb{P}[M]=\\mathbb{P}\\left[\\bigcup_{h\\in \\mathcal{H}_B}\\{{S|_{\\mathcal{X}}}:L_{(S,f)}(h)=0\\}\\right]$. So by the union bound we have:\n",
    "$$\n",
    "\\mathbb{P}\\left[\\exists h\\in \\mathcal{H} \\ s.t. \\ L_{(\\mathcal{\\overline{D}_m,f})}(h)>\\varepsilon \\ and \\ L_{(S,f)}(h)=0\\right]\\leq |\\mathcal{H}|\\times \\mathbb{P}\\left[\\{{S|_{\\mathcal{X}}}:L_{(S,f)}(h)=0\\}\\right] \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)\n",
    "$$\n",
    "\n",
    "On the other hand, if there exists $h\\in \\mathcal{H}$ such that $L_{(\\mathcal{\\overline{D}_m,f})}(h)>\\varepsilon$ then by definition of the generalization error we have:\n",
    "$$\\frac{\\mathbb{P}_{{x}\\sim\\mathcal{D}_i}[h(x)\\not=f(x)]+\\cdots+\\mathbb{P}_{{x}\\sim\\mathcal{D}_m}[h(x)\\not=f(x)]}{m}>\\varepsilon$$\n",
    "Therefore\n",
    "$$\\frac{\\mathbb{P}_{{x}\\sim\\mathcal{D}_i}[h(x)=f(x)]+\\cdots+\\mathbb{P}_{{x}\\sim\\mathcal{D}_m}[h(x)=f(x)]}{m}\\leq 1-\\varepsilon \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2)$$\n",
    "Also we have:\n",
    "$$\\mathbb{P}\\left[\\{{S|_{\\mathcal{X}}}:L_{(S,f)}(h)=0\\}\\right]=\\prod_{i=1}^m \\mathbb{P}_{{x}\\sim\\mathcal{D}_i}[h(x)=f(x)]=\\left(\\left(\\prod_{i=1}^m \\mathbb{P}_{{x}\\sim\\mathcal{D}_i}[h(x)=f(x)]\\right)^{1/m}\\right)^m$$\n",
    "So by geometric-arithmetic mean inequality and $(2)$ we have:\n",
    "$$\\mathbb{P}\\left[\\{{S|_{\\mathcal{X}}}:L_{(S,f)}(h)=0\\}\\right]{\\leq} \\left(\\frac{\\mathbb{P}_{{x}\\sim\\mathcal{D}_i}[h(x)=f(x)]+\\cdots+\\mathbb{P}_{{x}\\sim\\mathcal{D}_m}[h(x)=f(x)]}{m}\\right)^m\\leq  \\left(1-\\varepsilon\\right)^m\\leq e^{-\\varepsilon m}\n",
    "$$\n",
    "Therefore by $(1)$ we have:\n",
    "$$\\mathbb{P}\\left[\\exists h\\in \\mathcal{H} \\ s.t. \\ L_{(\\mathcal{\\overline{D}_m,f})}(h)>\\varepsilon \\ and \\ L_{(S,f)}(h)=0\\right]\\leq |\\mathcal{H}|e^{-\\varepsilon m}.$$\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $3.6$**\n",
    "\n",
    "Suppose that $\\mathcal{H}$ is agnostic PAC learnable. So there exist an algorithm $A$ and a function $m_{\\mathcal{H}}:(0,1)^2\\rightarrow \\mathbb{N}$ such that for every $\\varepsilon,\\delta\\in (0,1)$ and for every distribution $\\mathcal{D}$ over $\\mathcal{X}\\times \\mathcal{Y}$, when running $A$ on $m \\geq m_{\\mathcal{H}}(\\varepsilon,\\delta)$ i.i.d. examples generated by $\\mathcal{D}$, $A$ returns a hypothesis $h$ such that, with probability of at least $1-\\delta$ (over the choice of the $m$ training examples), $L_{\\mathcal{D}}(h)\\leq min_{h'\\in \\mathcal{H}}L_{\\mathcal{D}}(h')+\\varepsilon$.\n",
    "\n",
    "Now we want to show that if the realizability assumption holds, $\\mathcal{H}$ is PAC learnable using $A$. Let $\\mathcal{D}$ be a probability distribution over $\\mathcal{X}$ and $\\mathcal{f}$ be the target hypothesis in $\\mathcal{H}$. Consider the distribution $\\mathcal{D}'$ over $\\mathcal{X}\\times \\{0,1\\}$ obtained by drawing $x\\in \\mathcal{X}$ according to $\\mathcal{D}$ and taking the pair $(x,\\mathcal{f}(x))$. By the realizability assumption, $min_{h'\\in \\mathcal{H}}L_{\\mathcal{D}}(h')=0$. Let $\\varepsilon,\\delta\\in (0,1)$. Therefore when running $A$ on $m \\geq m_{\\mathcal{H}}(\\varepsilon,\\delta)$ i.i.d. examples which are labeled by $\\mathcal{f}$, $A$ returns a hypothesis $h$ such that, with probability of at least $1-\\delta$ (over the choice of the $m$ training examples) we have:\n",
    "$$L_{\\mathcal{D}}(h)\\leq min_{h'\\in \\mathcal{H}}L_{\\mathcal{D}}(h')+\\varepsilon=\\varepsilon.$$\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $3.7$**\n",
    "\n",
    "I am thinking...\n",
    "\n",
    "---------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
