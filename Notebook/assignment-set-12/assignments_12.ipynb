{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.2",
      "language": "python",
      "name": "py38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "assignments-12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zata213/Applied_Machine_Learning_S20_Assignments/blob/master/Notebook/assignment-set-12/assignments_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbCKLzYf3sbD",
        "colab_type": "text"
      },
      "source": [
        "#### Solutions of some exercises of the book \"Understanding Machine Learning-from Theory to Algorithms\"      \n",
        "by [Zahra Taheri](https://github.com/zata213/Applied_Machine_Learning_S20_Assignments) (12 June 2020)\n",
        "\n",
        "#### Chapter 11 (Model Selection and Validation)\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUBQg_pV3sbM",
        "colab_type": "text"
      },
      "source": [
        "**Exercise $11.1$**\n",
        " \n",
        "Consider a case in that the label is chosen at random according to $P[y = 1] = P[y = 0] = \\frac{1}{2}$. Suppose that if the parity of the labels on the training set $S\\subset \\mathcal{X}\\times \\mathcal{Y}$, i.e., $\\left(\\sum_{y\\in S|_{\\mathcal{Y}}}{y}\\right)\\text{mod} \\ 2$, is $1$, then the output of the learning algorithm $A$ is the constant predictor $h(x) = 1$ and otherwise the output is the constant predictor $h(x)=0$. \n",
        " \n",
        "Let $S$ be an i.i.d. sample and $h$ be the output of the algorithm on $S$. Since $h$ is a constant function, $L_\\mathcal{D}(h)=\\frac{1}{2}$.\n",
        "Now, we want to show that the leave-one-out estimate $L_V(h)$ is $1$. Without loss of generality, suppose that the parity of the labels on the training set is $1$. Then the parity of the labels on $S|_{\\mathcal{X}}\\setminus \\{x\\}$ is $1$ or $0$.\n",
        "- Let the parity of the labels on $S|_{\\mathcal{X}}\\setminus \\{x\\}$ is $1$. Then $y=0$ and the output of the learning algorithm $A$ on $S|_{\\mathcal{X}}\\setminus \\{x\\}$ is the constant function $g=1$. Also we have $L_{\\{(x,y)\\}}(g)=1$.\n",
        "- Let the parity of the labels on $S|_{\\mathcal{X}}\\setminus \\{x\\}$ is $0$. Then $y=1$. Also the output of the learning algorithm $A$ on $S|_{\\mathcal{X}}\\setminus \\{x\\}$ is the constant function $g=0$. Also we have $L_{\\{(x,y)\\}}(g)=1$.\n",
        " \n",
        "Therefore, the leave-one-out estimate $L_V(h)$ is $1$ and so the difference between the leave-one-out estimate and the true error in such a case is $\\frac{1}{2}$.\n",
        " \n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n13agao3sbY",
        "colab_type": "text"
      },
      "source": [
        "**Exercise $11.2$**\n",
        " \n",
        "Let $H_1,\\ldots,H_k$ be $k$ hypothesis classes and $S$  be an i.i.d. training set. We want to learn the class $H =\\cup_{i=1}^k H_i$ on $S$. \n",
        " \n",
        "Consider the case in that $H_1\\subseteq H_2\\subseteq \\ldots \\subseteq H_k$ and $|H_i|=2^i$, for all $i\\in [k]$. Then $H=H_k$. \n",
        " \n",
        "1. Since $H$ is a finite class then by Corollary $4.6$, $H$ is agnostically PAC learnable using the ERM algorithm with\n",
        "sample complexity $m\\leq \\lceil \\frac{2log(2|H|/\\delta)}{\\varepsilon^2} \\rceil$ and so we have\n",
        "$$L_D(h)\\leq min_{h'\\in H} L_D(h')+ \\sqrt{\\frac{2\\left( k+1+log(1/\\delta)\\right)}{m}}\\ \\ \\ \\ \\ (*)$$\n",
        "where $h$ is the output of the $ERM_H(S)$.\n",
        " \n",
        "2. Now suppose that we divide the $m$ examples into a training set of size $(1 − \\alpha)m$ and a validation set of size $\\alpha m$, for some $\\alpha \\in (0,1)$, and we apply the approach of model selection using validation. Suppose that $\\hat{h}_1,\\ldots, \\hat{h}_k$ be the resulting hypotheses obtained by training each class $H_i$ on the $(1 − \\alpha)m$ training examples using the ERM rule with respect to $H_i$. Then using Hoeffding’s inequality and the union bound, and so by Theorem $11.2$, for all $r\\in [k]$, with probability of at least $1-\\frac{\\delta}{2}$ over the choise of a validation set $V$ of size $\\alpha m$ we have \n",
        "$$ |L_D(\\hat{h}_r) − L_V(\\hat{h}_r)|\\leq \\sqrt{\\frac{log\\left(4k/\\delta)\\right)}{2\\alpha m}}.$$\n",
        "   Similarly if $\\hat{h}$ be the output of applying the ERM rule with respect to the finite class $\\{\\hat{h}_1,\\ldots, \\hat{h}_k\\}$ on the $\\alpha m$ validation examples, then with probability of at least $1-\\frac{\\delta}{2}$ over the choise of a validation set $V$ of size $\\alpha m$ we have \n",
        "$$ |L_D(\\hat{h}) − L_V(\\hat{h})|\\leq \\sqrt{\\frac{log\\left(4k/\\delta)\\right)}{2\\alpha m}}.$$   \n",
        "    Therefore, for all $r\\in [k]$ we have\n",
        "$$L_D(\\hat{h})\\leq L_V(\\hat{h})+\\sqrt{\\frac{log\\left(4k/\\delta)\\right)}{2\\alpha m}}\\leq L_V(\\hat{h}_r)+\\sqrt{\\frac{log\\left(4k/\\delta)\\right)}{2\\alpha m}}\\\\\n",
        "\\leq L_D(\\hat{h}_r)+2\\sqrt{\\frac{log\\left(4k/\\delta)\\right)}{2\\alpha m}}\n",
        "= L_D(\\hat{h}_r)+\\sqrt{\\frac{2log\\left(4k/\\delta)\\right)}{\\alpha m}}.$$\n",
        "    So, for all $r\\in [k]$\n",
        "$$L_D(\\hat{h})\\leq L_D(\\hat{h}_r)+\\sqrt{\\frac{2log\\left(4k/\\delta)\\right)}{\\alpha m}}.\\ \\ \\ \\ \\ \\ \\ \\ (1)$$\n",
        "    Now suppose that $j$ is the minimal index with the properties $h^*\\in H_j$ and $h^*\\in argmin_{h\\in H} L_D(h)$. By inequality $(1)$ we have \n",
        "    $$L_D(\\hat{h})\\leq L_D(\\hat{h}_j)+\\sqrt{\\frac{2log\\left(4k/\\delta)\\right)}{\\alpha m}}.\\ \\ \\ \\ \\ \\ \\ \\ \\ (2)$$\n",
        "    Since $\\hat{h}_j$ is the resulting hypotheses obtained by training the class $H_j$ on the $(1 − \\alpha)m$ training examples using the ERM rule with respect to $H_j$, with a discussion such as above by Theorem $11.2$, with probability of at least $1-\\frac{\\delta}{2}$ we have \n",
        "    $$L_D(\\hat{h}_j)\\leq L_D(h^*)+\\sqrt{\\frac{2log\\left(4|H_j|/\\delta)\\right)}{(1-\\alpha) m}}.\\ \\ \\ (3)$$\n",
        " \n",
        "    By the union bound and the inequalities $(2)$ and $(3)$, with probability of at least $1-\\delta$ we have \n",
        "    $$L_D(\\hat{h})\\leq L_D(h^*)+\\sqrt{\\frac{2log\\left(4k/\\delta)\\right)}{\\alpha m}}+\\sqrt{\\frac{2log\\left(4|H_j|/\\delta)\\right)}{(1-\\alpha) m}}=L_D(h^*)+\\sqrt{\\frac{2}{\\alpha m}log\\frac{4k}{\\delta}}+\\sqrt{\\frac{2}{(1-\\alpha) m}(j+log\\frac{4}{\\delta}})\\ \\ (**)$$\n",
        "    \n",
        "By the inequalities $(*)$ and $(**)$, the bound achieved using model selection is much better whenever $j$ is small enough compared to $k$.\n",
        "    \n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XkNdW0-3sbe",
        "colab_type": "text"
      },
      "source": [
        "**Referential Note:** After thinking on each exercise for at least one day, to solve **some parts** of exercises, I got hints from a [solution manual](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/exercises.html) of the book by Alon Gonen."
      ]
    }
  ]
}