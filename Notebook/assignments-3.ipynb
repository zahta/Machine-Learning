{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises of the book \"Understanding Machine Learning-from Theory to Algorithms\"\n",
    "\n",
    "## Chapter 2 (A Gentle Start)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $2.1$**\n",
    "\n",
    "Let $\\mathcal{f}$ be the labeling function. We want to show that given a training set $S$ as follows \n",
    "$$S=((x_i,\\mathcal{f}(x_i))) _{i=1}^m\\subseteq (\\mathbb{R}^d\\times \\{0,1\\}) ^m,$$\n",
    "there exists a polynomial $p_{_S}$ such that $h_{_S}(x)=1$ if and only if $p_{_S}(x)\\geq 0$, where $h_{_S}:\\mathcal{X}\\rightarrow \\mathcal{Y}$\n",
    "is the following predictor:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "h_{_S}(x)=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "y_i & \\mbox{if $\\exists i \\in [m]$ s. t. $x=x_i$}\\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{array} \n",
    " \\right.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where $[m]=\\{1,\\ldots,m\\}$ and $y_i=\\mathcal{f}(x_i)$, for all $i\\in [m]$.\n",
    "\n",
    "Obviously, we have:\n",
    "\\begin{eqnarray*}\n",
    "h_{_S}(x)=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\mbox{if $\\exists i \\in [m]$ s. t. $x=x_i$ and $y_i=1$}\\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{array} \n",
    " \\right.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "It is easy to see that if $y_i=0$, for all $i \\in [m]$, then $h_{_S}(x)=0$, for all $x\\in \\mathcal{X}$. In this case, if $p_{_S}(x):=-1$, for all $x\\in \\mathcal{X}$, then the statement is obviously true. \n",
    "\n",
    "Suppose that there exists an $i$ in $[m]$ such that $y_i=1$.\n",
    "\n",
    "1. Let $m=1$ and $S=((x_1,1))$. Then $h_{_S}(x_1)=1$ and $h_{_S}(x)=0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1\\}$. So we must have $p_{_S}(x_1)\\geq 0$ and $p_{_S}(x)<0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1\\}$. \n",
    "Let $p_{_S}(x):=-\\Vert x-x_1\\Vert^2$. Such a definition helps us to check the distance between $x$ and $x_1$. It is easy to see that $p_{_S}(x_1)=0$ and $p_{_S}(x)<0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1\\}$.\n",
    "\n",
    "2. Let $m=2$. Then without loss of generality we may assume that $S=((x_1,1),(x_2,0))$ or $S=((x_1,1),(x_2,1))$. \n",
    "   * If $S=((x_1,1),(x_2,0))$, then $h_{_S}(x_1)=1$ and $h_{_S}(x)=0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1\\}$. So, similar to the case (1), by defining $p_{_S}(x):=-\\Vert x-x_1\\Vert^2$, the statement is obviously true.\n",
    "   * If $S=((x_1,1),(x_2,1))$, then $h_{_S}(x_1)=h_{_S}(x_2)=1$ and $h_{_S}(x)=0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1,x_2\\}$. So we must have, $p_{_S}(x_1)\\geq 0$, $p_{_S}(x_2)\\geq 0$ and $p_{_S}(x)<0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1,x_2\\}$. \n",
    "With a similar discussion as above, let $p_{_S}(x):=-(\\Vert x-x_1\\Vert^2)(\\Vert x-x_1\\Vert^2)$. Then $p_{_S}(x)$ is a polynomial such that $p_{_S}(x_1)=p_{_S}(x_2)=0$ and $p_{_S}(x)<0$, for all $x\\in \\mathcal{X}\\setminus\\{x_1,x_2\\}$, and so the statement is obviously true. \n",
    "\n",
    "Inductively, we can generalize the obtained polynomials in cases (1) and (2) as follows\n",
    "$$p_{_S}(x):=-\\prod_{i\\in [m] \\text{    s. t.   } y_i=1}\\Vert x-x_i\\Vert^2$$\n",
    "Then $p_{_S}(x)$ is a polynomial such that $p_{_S}(x_i)=0$, for all $i \\in [m]$ such that $y_i=1$, and $p_{_S}(x)<0$, for all $x\\in \\mathcal{X}\\setminus\\{x_i|i\\in [m] \\text{ and } y_i=1\\}$, and so the statement is obviously true. \n",
    "\n",
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $2.2$**\n",
    "\n",
    "Let $\\mathcal{H}$ be a class of binary classifiers over a domain $\\mathcal{X}$ and $\\mathcal{D}$ be a probability distribution over $\\mathcal{X}$. Let $\\mathcal{f}$ be the target hypothesis in $\\mathcal{H}$. Let $S=((x_i,\\mathcal{f}(x_i))) _{i=1}^m$ be a training set such that $x_1,\\ldots,x_m$ are i.i.d with respect to $\\mathcal{D}$, denoted by $S\\sim\\mathcal{D}^m$. Also let $y_i=\\mathcal{f}(x_i)$, for all $i\\in [m]$. Fix some $h\\in\\mathcal{H}$.\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mathbb{E}_{S\\sim\\mathcal{D}^m}[L_{_S}(h)] & \\stackrel{*}{=} & \\dfrac{1}{m}\\sum_{i=1}^m\\mathbb{E}_{{x_i}\\sim{\\mathcal{D}}}[\\mathbb{1}_{h(x_i)\\not=f(x_i)}]\\\\\n",
    "& \\stackrel{**}{=} &\\dfrac{1}{m}\\sum_{i=1}^m\\mathbb{E}_{{x}\\sim\\mathcal{D}}[\\mathbb{1}_{h(x)\\not=f(x)}]\\\\\n",
    "& = & m \\left(\\frac{1}{m}\\right) \\mathbb{P}_{{x}\\sim\\mathcal{D}}[h(x)\\not=f(x)]\\\\\n",
    "& = & L_{(\\mathcal{D},\\mathcal{f})}[h]\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\\* Expectation is linear          \n",
    "\\** $x_1,\\ldots,x_m$ are i.i.d\n",
    "\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $2.3$**\n",
    "\n",
    "I am thinking...\n",
    "\n",
    "---------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
